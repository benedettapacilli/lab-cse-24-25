{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {
    "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
   },
   "source": [
    "# 103 Spark optimizations\n",
    "\n",
    "The goal of this lab is to understand some of the optimization mechanisms of Spark.\n",
    "\n",
    "- [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "- [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a037caa76dc389a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:44:34.323685500Z",
     "start_time": "2024-10-30T08:44:34.054363700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "import org.apache.spark\r\n"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504e515fbb5fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// DO NOT EXECUTE - this is needed just to avoid showing errors in the following cells\n",
    "val sc = spark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
   "metadata": {
    "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-30T08:27:59.758384300Z",
     "start_time": "2024-10-30T08:27:56.678713400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "parseWeather: (row: String)(String, String, String, String, String, Int, Boolean)\r\nparseStation: (row: String)(String, String, String, String, String, Double, Double, Double, String, String)\r\n"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// WEATHER structure: (usaf,wban,year,month,day,airTemperature,airTemperatureQuality)\n",
    "def parseWeather(row:String) = {\n",
    "    val usaf = row.substring(4,10)\n",
    "    val wban = row.substring(10,15)\n",
    "    val year = row.substring(15,19)\n",
    "    val month = row.substring(19,21)\n",
    "    val day = row.substring(21,23)\n",
    "    val airTemperature = row.substring(87,92)\n",
    "    val airTemperatureQuality = row.charAt(92)\n",
    "\n",
    "    (usaf,wban,year,month,day,airTemperature.toInt/10,airTemperatureQuality == '1')\n",
    "}\n",
    "\n",
    "// STATION structure: (usaf,wban,city,country,state,latitude,longitude,elevation,date_begin,date_end) \n",
    "def parseStation(row:String) = {\n",
    "    def getDouble(str:String) : Double = {\n",
    "        if (str.isEmpty)\n",
    "            return 0\n",
    "        else\n",
    "            return str.toDouble\n",
    "    }\n",
    "    val columns = row.split(\",\").map(_.replaceAll(\"\\\"\",\"\"))\n",
    "    val latitude = getDouble(columns(6))\n",
    "    val longitude = getDouble(columns(7))\n",
    "    val elevation = getDouble(columns(8))\n",
    "    (columns(0),columns(1),columns(2),columns(3),columns(4),latitude,longitude,elevation,columns(9),columns(10))  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c70c02bd-4c8f-4cc2-9a13-544da7c6544d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:36:20.894360700Z",
     "start_time": "2024-10-30T09:36:19.686789300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "rddWeather: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)] = MapPartitionsRDD[141] at map at <console>:41\r\nrddStation: org.apache.spark.rdd.RDD[(String, String, String, String, String, Double, Double, Double, String, String)] = MapPartitionsRDD[144] at map at <console>:44\r\n"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddWeather = sc.\n",
    "  textFile(\"../../../../datasets/big/weather-sample1.txt\").\n",
    "  map(x => parseWeather(x))\n",
    "val rddStation = sc.\n",
    "  textFile(\"../../../../datasets/weather-stations.csv\").\n",
    "  map(x => parseStation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b49ee-6852-4025-9e55-3950ff937680",
   "metadata": {
    "id": "ef4b49ee-6852-4025-9e55-3950ff937680"
   },
   "source": [
    "## 103-1 Simple job optimization\n",
    "\n",
    "Optimize the two jobs (avg temperature and max temperature) by avoiding the repetition of the same computations and by enforcing a partitioning criteria.\n",
    "- There are multiple methods to repartition an RDD: check the ```coalesce```, ```partitionBy```, and ```repartition``` methods on the documentation and choose the best one.\n",
    "  - To create a partitioning function, you must ```import org.apache.spark.HashPartitioner``` and then define ```val p = new HashPartitioner(n)``` where ```n``` is the number of partitions to create\n",
    "- Verify your persisted data in the web UI\n",
    "- Verify the execution plan of your RDDs with ```rdd.toDebugString``` (shell only) or on the web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "import org.apache.spark.HashPartitioner\r\nn: Int = 10\r\np: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@a\r\n"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val n = 10\n",
    "val p = new HashPartitioner(n)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-30T08:28:33.229972300Z",
     "start_time": "2024-10-30T08:28:30.922550600Z"
    }
   },
   "id": "4642cc86eef78605"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
   "metadata": {
    "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-30T09:16:02.896250Z",
     "start_time": "2024-10-30T09:15:52.912223100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "AverageTemperatureByMonth: (rdd: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)])Array[(String, Double)]\r\nres9: Array[(String, Double)] = Array((10,13.32), (11,8.15), (12,4.08), (01,3.06), (02,5.5), (03,8.31), (04,11.75), (05,15.83), (06,18.53), (07,19.96), (08,20.31), (09,17.24))\r\n"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Average temperature for every month\n",
    "def AverageTemperatureByMonth(rdd:org.apache.spark.rdd.RDD[(String,String,String,String,String,Int,Boolean)]) = {\n",
    "    rdd.\n",
    "        filter(_._6<999).\n",
    "        map(x => (x._4, x._6)).\n",
    "        aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2)).\n",
    "        map({case(k,v)=>(k,Math.round(v._1*100/v._2)/100.0)}).\n",
    "        collect()\n",
    "}\n",
    "\n",
    "AverageTemperatureByMonth(rddWeather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "34: error: type mismatch;\r",
     "output_type": "error",
     "traceback": [
      "<console>:34: error: type mismatch;\r",
      " found   : ((Double, Double), Double) => (Double, Double)\r",
      " required: ((Double, Double), Int) => (Double, Double)\r",
      "           (a: (Double, Double), v: Double) => (a._1 + v, a._2 + 1),\r",
      "                                            ^\r",
      "<console>:37: error: value _1 is not a member of Any\r",
      "         .map { case (k, v) => (k, Math.round(v._1 * 100 / v._2) / 100.0) }\r",
      "                                                ^\r",
      "<console>:37: error: value _2 is not a member of Any\r",
      "         .map { case (k, v) => (k, Math.round(v._1 * 100 / v._2) / 100.0) }\r",
      "                                                             ^\r",
      ""
     ]
    }
   ],
   "source": [
    "// optimized average temperature\n",
    "val partitionedRDD = rddWeather\n",
    "  .filter(_._6 < 999)\n",
    "  .map(x => (x._4, x._6))\n",
    "  .repartition(10)\n",
    "  .cache()\n",
    "\n",
    "val avgTempByMonth = partitionedRDD\n",
    "  .aggregateByKey((0.0, 0.0))(\n",
    "    (a: (Double, Double), v: Double) => (a._1 + v, a._2 + 1),\n",
    "    (a1: (Double, Double), a2: (Double, Double)) => (a1._1 + a2._1, a1._2 + a2._2)\n",
    "  )\n",
    "  .map { case (k, v) => (k, Math.round(v._1 * 100 / v._2) / 100.0) }\n",
    "  .collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-30T08:29:16.145750700Z",
     "start_time": "2024-10-30T08:29:15.299206Z"
    }
   },
   "id": "77d29f28aa8a576f"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b614d5393d1a1c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:16:39.880325600Z",
     "start_time": "2024-10-30T09:16:32.218427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "MaxTemperatureByMonth: (rdd: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)])Array[(String, Int)]\r\nres10: Array[(String, Int)] = Array((10,55), (11,43), (12,47), (01,55), (02,47), (03,44), (04,48), (05,49), (06,56), (07,56), (08,56), (09,55))\r\n"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Maximum temperature for every month\n",
    "def MaxTemperatureByMonth(rdd:org.apache.spark.rdd.RDD[(String,String,String,String,String,Int,Boolean)]) = {\n",
    "    rdd.\n",
    "        filter(_._6<999).\n",
    "        map(x => (x._4, x._6)).\n",
    "        reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "        collect()\n",
    "}\n",
    "\n",
    "MaxTemperatureByMonth(rddWeather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "OptimizedMaxTemperatureByMonth: (rdd: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)])Array[(String, Int)]\r\n"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Optimized maximum temperature for every month\n",
    "def OptimizedMaxTemperatureByMonth(rdd:org.apache.spark.rdd.RDD[(String,String,String,String,String,Int,Boolean)]) = {\n",
    "    rdd\n",
    "      .filter(_._6<999)\n",
    "      .map(x => (x._4, x._6))\n",
    "      .reduceByKey((x,y)=>{if(x<y) y else x})\n",
    "      .repartition(10)\n",
    "      .cache()\n",
    "      .collect()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-30T09:30:20.370735100Z",
     "start_time": "2024-10-30T09:30:20.047737400Z"
    }
   },
   "id": "86e6a2eca0b558bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OptimizedMaxTemperatureByMonth(rddWeather)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab3b2cff2454a6b7"
  },
  {
   "cell_type": "markdown",
   "id": "377fbf30-f568-413c-9238-de139db23135",
   "metadata": {
    "id": "377fbf30-f568-413c-9238-de139db23135"
   },
   "source": [
    "## 103-2 RDD preparation\n",
    "\n",
    "Check the five possibilities to prepare the Station RDD for subsequent processing and identify the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
   "metadata": {
    "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-30T08:31:10.060234700Z",
     "start_time": "2024-10-30T08:31:08.916489700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "import org.apache.spark.HashPartitioner\r\np2: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\r\nrddS1: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[29] at map at <console>:37\r\nrddS2: org.apache.spark.rdd.RDD[(String, (String, Double))] = ShuffledRDD[32] at partitionBy at <console>:42\r\nrddS3: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[35] at map at <console>:46\r\nrddS4: org.apache.spark.rdd.RDD[(String, (String, Double))] = ShuffledRDD[38] at partitionBy at <console>:51\r\nrddS5: org.apache.spark.rdd.RDD[(String, (String, Double))] = ShuffledRDD[40] at partitionBy at <console>:55\r\n"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val p2 = new HashPartitioner(8)\n",
    "\n",
    "// _1 and _2 are the fields composing the key; _4 and _8 are country and elevation, respectively\n",
    "val rddS1 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p2).\n",
    "  cache().\n",
    "  map({case (k,v) => (k,(v._4,v._8))}) // mapping after the partition breaks the partitioning criteria\n",
    "val rddS2 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  cache().\n",
    "  partitionBy(p2) // partitioning after the caching breaks the partitioning criteria\n",
    "val rddS3 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}). // mapping before the caching and partitioning criteria\n",
    "  cache()\n",
    "val rddS4 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  partitionBy(p2).\n",
    "  cache()\n",
    "val rddS5 = rddStation.\n",
    "  map(x => (x._1 + x._2, (x._4,x._8))).\n",
    "  partitionBy(p2). // partitioning before the caching and after the mapping criteria\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "t1: Long = 1284976439000\r\nt2: Long = 1285138099500\r\nt3: Long = 1285138106400\r\nt4: Long = 1285526429200\r\nt5: Long = 1285526442700\r\nt6: Long = 1286194044800\r\nt7: Long = 1286194056200\r\nt8: Long = 1286547294200\r\nt9: Long = 1286547305900\r\ntimes: List[Long] = List(161660500, 388322800, 667602100, 353238000, 11700)\r\nres4: (Long, Int) = (11700,4)\r\n"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t1 = System.nanoTime()\n",
    "rddS1.collect()\n",
    "val t2 = System.nanoTime()\n",
    "val t3 = System.nanoTime()\n",
    "rddS2.collect()\n",
    "val t4 = System.nanoTime()\n",
    "val t5 = System.nanoTime()\n",
    "rddS3.collect()\n",
    "val t6 = System.nanoTime()\n",
    "val t7 = System.nanoTime()\n",
    "rddS4.collect()\n",
    "val t8 = System.nanoTime()\n",
    "val t9 = System.nanoTime()\n",
    "rddS5.collect()\n",
    "\n",
    "val times = List(t2-t1,t4-t3,t6-t5,t8-t7,t9-t8)\n",
    "times.zipWithIndex.min"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-30T08:33:50.877027400Z",
     "start_time": "2024-10-30T08:33:48.142958500Z"
    }
   },
   "id": "bb65156157d54a2f"
  },
  {
   "cell_type": "markdown",
   "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed",
   "metadata": {
    "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed"
   },
   "source": [
    "## 103-3 Joining RDDs\n",
    "\n",
    "Define the join between rddWeather and rddStation and compute:\n",
    "- The maximum temperature for every city\n",
    "- The maximum temperature for every city in the UK: \n",
    "  - ```StationData.country == \"UK\"```\n",
    "- Sort the results by descending temperature\n",
    "  - ```map({case(k,v)=>(v,k)})``` to invert key with value and vice versa\n",
    "\n",
    "Hints & considerations:\n",
    "- Keep only temperature values <999\n",
    "- Join syntax: ```rdd1.join(rdd2)```\n",
    "  - Both RDDs should be structured as key-value RDDs with the same key: usaf + wban\n",
    "- Consider partitioning and caching to optimize the join\n",
    "  - Careful: it is not enough for the two RDDs to have the same number of partitions; they must have the same partitioner!\n",
    "- Verify the execution plan of the join in the web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "sc.getPersistentRDDs.foreach(_._2.unpersist()) // this command cleans the cache"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-30T10:16:47.187069400Z",
     "start_time": "2024-10-30T10:16:46.978593400Z"
    }
   },
   "id": "eaa3d6e4e68721e6"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "rddWeatherPairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[328] at map at <console>:45\r\nrddStationPairs: org.apache.spark.rdd.RDD[(String, (String, String))] = MapPartitionsRDD[329] at map at <console>:47\r\nrddCached: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[334] at reduceByKey at <console>:53\r\n"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddWeatherPairs = rddWeather\n",
    "  .filter(r => r._6 < 999) //don't cache datasets when iterating over them only once, cache them if you need to do multiple operations on them\n",
    "  .map(r => (r._1 + r._2, r._6)) // key is usaf + wban, value is airTemperature\n",
    "val rddStationPairs = rddStation\n",
    "  .map(r => (r._1 + r._2, (r._3, r._4))) // key is usaf + wban, value is (city, country)\n",
    "\n",
    "val rddCached = rddWeatherPairs\n",
    "  .join(rddStationPairs)\n",
    "  .map(r => ((r._2._2._1, r._2._2._2), r._2._1)) // key is city + country, value is airTemperature \n",
    "  // (city, country) -> airTemperature is more space efficient than (city) -> airTemperature, country\n",
    "  .reduceByKey((x,y) => {if(x < y) y else x}) // maximum temperature for every city\n",
    "  .cache() // cache the result of the join"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-30T10:41:17.771638900Z",
     "start_time": "2024-10-30T10:41:17.473430100Z"
    }
   },
   "id": "871a092fc8b0c8d6"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "res53: Array[((String, String), Int)] = Array(((MUSTASAARI VALASSAARET,FI),20), ((LOCH GLASCARNOCH,UK),22), ((KAJAANI,FI),30), ((FOULA,UK),15), ((TULLOCH BRIDGE,UK),26), ((RACKWICK,UK),19), ((INVERGORDON HARBOUR,UK),20), ((BUTT OF LEWIS (LH),UK),8), ((KIRKWALL,UK),19), ((VARKAUS,FI),29), ((SELLA NESS,UK),19), ((LOCHBOISDALE,UK),21), ((HAILUOTO ISLAND,FI),28), ((LEMLAND NYHAMN,FI),19), ((INKOO BAGASKAR,FI),22), ((BARRA ISLAND,UK),21), ((FOULA NO2,UK),15), ((BENBECULA,UK),20), ((KAUHAVA,FI),29), ((GLENLIVET,UK),23), ((AHTARI MYLLYMAKI,FI),28), ((JOMALA,FI),22), ((KRUUNUPYY,FI),28), ((WATERSTEIN,UK),21), ((KINLOSS,UK),24), ((JYVASKYLA,FI),29), ((HANKO RUSSARO,FI),22), ((SUMBURGH,UK),30), ((HALLI,FI),29), ((CAIRNGORM SUMMIT,UK),14), ((NORTH RONA ISLAND,UK),21), ((LAPPEENRANTA HIEKKAPAKKA,FI...\r\n"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCached.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-30T10:41:22.421447800Z",
     "start_time": "2024-10-30T10:41:19.363762500Z"
    }
   },
   "id": "58842e2280a5ac8e"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "res56: Array[(Int, (String, String))] = Array((34,(SOUTH UIST RANGE,UK)), (30,(SUMBURGH,UK)), (26,(TULLOCH BRIDGE,UK)), (24,(KINLOSS,UK)), (24,(ALTNAHARRA NO2,UK)), (23,(GLENLIVET,UK)), (23,(LOSSIEMOUTH,UK)), (23,(AULTBEA NO2,UK)), (23,(FOYERS,UK)), (23,(AVIEMORE,UK)), (23,(INVERNESS,UK)), (23,(SKYE/LUSA,UK)), (22,(LOCH GLASCARNOCH,UK)), (22,(TAIN RANGE (SAWS),UK)), (21,(LOCHBOISDALE,UK)), (21,(BARRA ISLAND,UK)), (21,(WATERSTEIN,UK)), (21,(NORTH RONA ISLAND,UK)), (21,(KILMORY,UK)), (20,(INVERGORDON HARBOUR,UK)), (20,(BENBECULA,UK)), (20,(STORNOWAY,UK)), (20,(SCATSTA,UK)), (19,(RACKWICK,UK)), (19,(KIRKWALL,UK)), (19,(SELLA NESS,UK)), (18,(BALTASOUND NO.2,UK)), (17,(SULE SKERRY,UK)), (17,(AONACH MOR,UK)), (17,(LERWICK,UK)), (16,(MUCKLE HOLM,UK)), (15,(FOULA,UK)), (15,(FOULA NO2,UK)), (15,...\r\n"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCached\n",
    "  .filter(r => r._1._2 == \"UK\")\n",
    "  .map({case (k,v) => (v,k)}) // invert key with value so that SortByKey can sort by temperature\n",
    "  .sortByKey(false) // sort by descending temperature\n",
    "  .collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-30T10:47:40.261300200Z",
     "start_time": "2024-10-30T10:47:39.629126800Z"
    }
   },
   "id": "6c9ddce05e1d0d53"
  },
  {
   "cell_type": "markdown",
   "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882",
   "metadata": {
    "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882"
   },
   "source": [
    "## 103-4 Memory occupation\n",
    "\n",
    "Use Spark's web UI to verify the space occupied by the provided RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
   "metadata": {
    "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-30T10:51:17.224342200Z",
     "start_time": "2024-10-30T10:51:17.142746800Z"
    }
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "60: error: value persist is not a member of Array[(String, String, String, String, String, Int, Boolean)]\r",
     "output_type": "error",
     "traceback": [
      "<console>:60: error: value persist is not a member of Array[(String, String, String, String, String, Int, Boolean)]\r",
      "       val memSerRdd = memRdd.map(x=>x).persist(MEMORY_ONLY_SER)\r",
      "                                        ^\r",
      "<console>:61: error: value persist is not a member of Array[(String, String, String, String, String, Int, Boolean)]\r",
      "       val diskRdd = memRdd.map(x=>x).persist(DISK_ONLY)\r",
      "                                      ^\r",
      ""
     ]
    }
   ],
   "source": [
    "// simply running this does not show anything on \"Storage\", because cache is lazy\n",
    "import org.apache.spark.storage.StorageLevel._\n",
    "\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val memRdd = rddWeather.cache()\n",
    "val memSerRdd = memRdd.map(x=>x).persist(MEMORY_ONLY_SER)\n",
    "val diskRdd = memRdd.map(x=>x).persist(DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "res60: (String, String, String, String, String, Int, Boolean) = (028690,99999,2000,04,01,999,false)\r\n"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memRdd.first()\n",
    "memSerRdd.first()\n",
    "diskRdd.first()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-30T10:50:27.458139700Z",
     "start_time": "2024-10-30T10:50:27.206504500Z"
    }
   },
   "id": "3a428bcad030b0a7"
  },
  {
   "cell_type": "markdown",
   "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d",
   "metadata": {
    "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d"
   },
   "source": [
    "## 103-5 Evaluating different join methods\n",
    "\n",
    "Consider the following scenario:\n",
    "- We have a disposable RDD of Weather data (i.e., it is used only once): ```rddW```\n",
    "- And we have an RDD of Station data that is used many times: ```rddS```\n",
    "- Both RDDs are cached (```collect()```is called to enforce caching)\n",
    "\n",
    "We want to join the two RDDS. Which option is best?\n",
    "- Simply join the two RDDs\n",
    "- Enforce on ```rddW1``` the same partitioner of ```rddS``` (and then join)\n",
    "- Exploit broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d77122-8bdd-4784-a86e-f42f2da06759",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:18:53.686421Z",
     "start_time": "2024-10-20T16:18:51.290892Z"
    },
    "id": "31d77122-8bdd-4784-a86e-f42f2da06759",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val p = new HashPartitioner(8)\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val rddW = rddWeather.\n",
    "  filter(_._6<999).\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  persist()\n",
    "val rddS = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  cache()\n",
    "\n",
    "// Collect to enforce caching\n",
    "rddW.collect()\n",
    "rddS.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6822816cd65d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:19:05.603687Z",
     "start_time": "2024-10-20T16:19:04.730871Z"
    }
   },
   "outputs": [],
   "source": [
    "// Is it better to simply join the two RDDs..\n",
    "rddW.\n",
    "  join(rddS).\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e5f9827be45d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:19:26.072648Z",
     "start_time": "2024-10-20T16:19:25.244226Z"
    }
   },
   "outputs": [],
   "source": [
    "// ..to enforce on rddW1 the same partitioner of rddS..\n",
    "rddW.\n",
    "  partitionBy(p).\n",
    "  join(rddS).\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b618652ac67fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:19:36.264001Z",
     "start_time": "2024-10-20T16:19:35.988801Z"
    }
   },
   "outputs": [],
   "source": [
    "// ..or to exploit broadcast variables?\n",
    "val bRddS = sc.broadcast(rddS.map(x => (x._1, x._2._3)).collectAsMap())\n",
    "val rddJ = rddW.\n",
    "  map({case (k,v) => (bRddS.value.get(k),v._6)}).\n",
    "  filter(_._1!=None)\n",
    "rddJ.\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33",
   "metadata": {
    "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33"
   },
   "source": [
    "## 103-6 Optimizing Exercise 3\n",
    "\n",
    "Start from the result of the last job of Exercise 3; is there a more efficient way to compute the same result?\n",
    "- Try it on weather-sample10\n",
    "- Hint: consider that each station is located in only one country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47748353-fb4b-432f-af79-d1136453b956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:39:53.616157Z",
     "start_time": "2024-10-20T16:39:17.055562Z"
    },
    "id": "47748353-fb4b-432f-af79-d1136453b956",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "import org.apache.spark.storage.StorageLevel._\n",
    "val p = new HashPartitioner(8)\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val rddS = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  cache()\n",
    "val rddW = rddWeather.\n",
    "  filter(_._6<999).\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  persist(MEMORY_AND_DISK_SER)\n",
    "\n",
    "// Collect to enforce caching\n",
    "rddW.collect()\n",
    "rddS.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f448cc-efc7-4793-a3a2-4a19e0e6fc15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:29:51.299610Z",
     "start_time": "2024-10-20T16:29:50.682782Z"
    },
    "id": "67f448cc-efc7-4793-a3a2-4a19e0e6fc15",
    "tags": []
   },
   "outputs": [],
   "source": [
    "// First version\n",
    "rddW.\n",
    "  join(rddS).\n",
    "  filter(_._2._2._4==\"UK\").\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  map({case(k,v)=>(v,k)}).\n",
    "  sortByKey(false).\n",
    "  collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "302-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
